{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c65c32d",
   "metadata": {},
   "source": [
    "## 딥페이크 범죄 대응을 위한 AI 탐지 모델 경진대회\n",
    "\n",
    "**※주의** : 반드시 본 파일을 이용하여 제출을 수행해야 하며, 파일의 이름은 `task.ipynb`로 유지되어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a9781",
   "metadata": {},
   "source": [
    "* #### 추론 실행 환경\n",
    "    * `python 3.9` 환경\n",
    "    * `CUDA 10.2`, `CUDA 11.8`, `CUDA 12.6`를 지원합니다.\n",
    "    * 각 CUDA 환경에 미리 설치돼있는 torch 버전은 다음 표를 참고하세요.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th align=\"center\">Python</th>\n",
    "      <th align=\"center\">CUDA</th>\n",
    "      <th align=\"center\">torch</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td align=\"center\" style=\"vertical-align: middle;\">3.8</td>\n",
    "      <td align=\"center\">10.2</td>\n",
    "      <td align=\"center\">1.6.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\" style=\"vertical-align: middle;\">3.9</td>\n",
    "      <td align=\"center\">11.8</td>\n",
    "      <td align=\"center\">1.8.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">3.10</td>\n",
    "      <td align=\"center\">12.6</td>\n",
    "      <td align=\"center\">2.7.1</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001744a",
   "metadata": {},
   "source": [
    "* #### CUDA 버전 관련 안내사항  \n",
    "  - 이번 경진대회는 3개의 CUDA 버전을 지원합니다.  \n",
    "  - 참가자는 자신의 모델의 라이브러리 의존성에 맞는 CUDA 환경을 선택하여 모델을 제출하면 됩니다.   \n",
    "  - 각 CUDA 환경에는 기본적으로 torch가 설치되어 있으나, 참가자는 제출하는 CUDA 버전과 호환되는 torch, 필요한 버전의 라이브러리를 `!pip install` 하여 사용하여도 무관합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b530b5",
   "metadata": {},
   "source": [
    "* #### `task.ipynb` 작성 규칙\n",
    "코드는 크게 3가지 파트로 구성되며, 해당 파트의 특성을 지켜서 내용을 편집하세요.   \n",
    "1. **제출용 aifactory 라이브러리 및 추가 필요 라이브러리 설치**\n",
    "    - 채점 및 제출을 위한 aifactory 라이브러리를 설치하는 셀입니다. 이 부분은 수정하지 않고 그대로 실행합니다.\n",
    "    - 그 외로, 모델 추론에 필요한 라이브러리를 직접 설치합니다.\n",
    "2. **추론용 코드 작성**\n",
    "    - 모델 로드, 데이터 전처리, 예측 등 실제 추론을 수행하는 모든 코드를 이 영역에 작성합니다.\n",
    "3. **aif.submit() 함수를 호출하여 최종 결과를 제출**\n",
    "    - **마이 페이지-활동히스토리**에서 발급받은 key 값을 함수의 인자로 정확히 입력해야 합니다.\n",
    "    - **※주의** : 제출하고자 하는 CUDA 환경에 맞는 key를 입력하여야 합니다.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th align=\"left\">Competition 이름</th>\n",
    "      <th align=\"center\">CUDA</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td align=\"left\">딥페이크 범죄 대응을 위한 AI 탐지 모델 경진대회</td>\n",
    "      <td align=\"center\">11.8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"left\">딥페이크 범죄 대응을 위한 AI 탐지 모델 경진대회 CUDA 12.6</td>\n",
    "      <td align=\"center\">12.6</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"left\">딥페이크 범죄 대응을 위한 AI 탐지 모델 경진대회 CUDA 10.2</td>\n",
    "      <td align=\"center\">10.2</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8e0843",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e2061",
   "metadata": {},
   "source": [
    "#### 1. 제출용 aifactory 라이브러리 설치\n",
    "※ 결과 전송에 필요하므로 아래와 같이 aifactory 라이브러리가 반드시 최신버전으로 설치될 수 있게끔 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5f6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4acd91eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aifactory in /usr/local/lib/python3.12/dist-packages (2.0.0)\n",
      "Requirement already satisfied: pipreqs in /usr/local/lib/python3.12/dist-packages (from aifactory) (0.5.0)\n",
      "Requirement already satisfied: ipynbname in /usr/local/lib/python3.12/dist-packages (from aifactory) (2025.8.0.0)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (from aifactory) (5.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from aifactory) (2.32.5)\n",
      "Requirement already satisfied: IPython in /usr/local/lib/python3.12/dist-packages (from aifactory) (8.12.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown->aifactory) (4.14.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown->aifactory) (3.20.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown->aifactory) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown->aifactory) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown->aifactory) (4.15.0)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from ipynbname->aifactory) (6.30.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->ipynbname->aifactory) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.12/dist-packages (from ipykernel->ipynbname->aifactory) (1.8.17)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->ipynbname->aifactory) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->ipynbname->aifactory) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->ipynbname->aifactory) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /usr/local/lib/python3.12/dist-packages (from ipykernel->ipynbname->aifactory) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.12/dist-packages (from ipykernel->ipynbname->aifactory) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /usr/local/lib/python3.12/dist-packages (from ipykernel->ipynbname->aifactory) (7.1.0)\n",
      "Requirement already satisfied: pyzmq>=25 in /usr/local/lib/python3.12/dist-packages (from ipykernel->ipynbname->aifactory) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.12/dist-packages (from ipykernel->ipynbname->aifactory) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->ipynbname->aifactory) (5.14.3)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from IPython->aifactory) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from IPython->aifactory) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from IPython->aifactory) (0.19.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from IPython->aifactory) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.12/dist-packages (from IPython->aifactory) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from IPython->aifactory) (2.19.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.12/dist-packages (from IPython->aifactory) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from IPython->aifactory) (4.9.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython->aifactory) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->IPython->aifactory) (0.8.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=8.0.0->ipykernel->ipynbname->aifactory) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->ipynbname->aifactory) (4.5.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->IPython->aifactory) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel->ipynbname->aifactory) (1.16.0)\n",
      "Requirement already satisfied: docopt==0.6.2 in /usr/local/lib/python3.12/dist-packages (from pipreqs->aifactory) (0.6.2)\n",
      "Requirement already satisfied: nbconvert<8.0.0,>=7.11.0 in /usr/local/lib/python3.12/dist-packages (from pipreqs->aifactory) (7.16.6)\n",
      "Requirement already satisfied: yarg==0.1.9 in /usr/local/lib/python3.12/dist-packages (from pipreqs->aifactory) (0.1.9)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (3.1.6)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (3.0.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.12/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (1.5.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (4.25.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.27.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->aifactory) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->aifactory) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->aifactory) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->aifactory) (2025.10.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->aifactory) (1.7.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from stack-data->IPython->aifactory) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from stack-data->IPython->aifactory) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.12/dist-packages (from stack-data->IPython->aifactory) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U aifactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0deb93f",
   "metadata": {},
   "source": [
    "* 자신의 모델 추론 실행에 필요한 추가 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ceb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대회 서버 환경: Python 3.10 + CUDA 12.6 + torch 2.7.1 (기본 설치)\n",
    "# 커스텀 ViT 모델에 필요한 라이브러리 설치\n",
    "!pip install pandas --no-cache-dir --quiet\n",
    "!pip install timm==0.9.12 --no-cache-dir --quiet\n",
    "!pip install einops --no-cache-dir --quiet\n",
    "!pip install torchvision --no-cache-dir --quiet\n",
    "!pip install numpy --no-cache-dir --quiet\n",
    "!pip install opencv-python-headless --no-cache-dir --quiet\n",
    "!pip install Pillow --no-cache-dir --quiet\n",
    "!pip install dlib --no-cache-dir --quiet\n",
    "!pip install tqdm --no-cache-dir --quiet\n",
    "!pip install mediapipe --no-cache-dir --quiet\n",
    "\n",
    "# 리소스 모니터링 (대회 환경 디버깅용)\n",
    "!pip install psutil --no-cache-dir --quiet\n",
    "\n",
    "# LLaVA 추론에 필요한 라이브러리 설치\n",
    "!pip install transformers --no-cache-dir --quiet\n",
    "!pip install accelerate --no-cache-dir --quiet\n",
    "!pip install safetensors --no-cache-dir --quiet\n",
    "!pip install sentencepiece --no-cache-dir --quiet\n",
    "!pip install protobuf --no-cache-dir --quiet\n",
    "\n",
    "# LLaVA 라이브러리 설치 (GitHub에서 직접 설치)\n",
    "!pip install git+https://github.com/haotian-liu/LLaVA.git --no-cache-dir --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19999e24",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1b43e",
   "metadata": {},
   "source": [
    "#### 2. 추론용 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d194e",
   "metadata": {},
   "source": [
    "##### 추론 환경의 기본 경로 구조\n",
    "\n",
    "- 평가 데이터셋 경로: `./data/`\n",
    "   - 채점에 사용될 테스트 데이터셋은 `./data/` 디렉토리 안에 포함되어 있습니다.\n",
    "   - 해당 디렉토리에는 이미지(JPG, PNG)와 동영상(MP4) 파일이 별도의 하위 폴더 없이 혼합되어 있습니다.\n",
    "```bash\n",
    "/aif/\n",
    "└── data/\n",
    "    ├── {이미지 데이터1}.jpg\n",
    "    ├── {이미지 데이터2}.png\n",
    "    ├── {동영상 데이터1}.mp4\n",
    "    ├── {이미지 데이터3}.png\n",
    "    ├── {동영상 데이터2}.mp4\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335719c1",
   "metadata": {},
   "source": [
    "- 모델 및 자원 경로: 예시 : `./model/`\n",
    "   - 추론 스크립트가 실행되는 위치를 기준으로, 제출된 모델 관련 파일들이 위치해야하 하는 상대 경로입니다.\n",
    "   - 학습된 모델 가중치(.pt, .ckpt, .pth 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1810d",
   "metadata": {},
   "source": [
    "* 제출 파일은 `submission.csv`로 저장돼야 합니다.\n",
    "  * submission.csv는 *filename*과 *label* 컬럼으로 구성돼야 합니다.\n",
    "  * filename은 추론한 파일의 이름(확장자 포함), label은 추론 결과입니다. (real:0, fake:1)\n",
    "  * filename은 *string*, label은 *int* 자료형이어야 합니다.\n",
    "  * 추론하는 데이터의 순서는 무작위로 섞여도 상관 없습니다.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th align=\"center\">filename</th>\n",
    "      <th align=\"center\">label</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td align=\"center\">{이미지 데이터1}.jpg</td>\n",
    "      <td align=\"center\">0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">{동영상 데이터1}.mp4</td>\n",
    "      <td align=\"center\">1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td colspan=\"2\" align=\"center\">...</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c29c10f",
   "metadata": {},
   "source": [
    "**※ 주의 사항**\n",
    "\n",
    "* argparse 사용시 `args, _ = parser.parse_known_args()`로 인자를 지정하세요.   \n",
    "   - `args = parser.parse_args()`는 jupyter에서 오류가 발생합니다.\n",
    "* return 할 결과물과 양식에 유의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import dlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# stdout/stderr 에러 억제\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Mediapipe 및 TensorFlow Lite 로그 억제 (import 전에 설정)\n",
    "os.environ['GLOG_minloglevel'] = '3'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# ============================================================\n",
    "# 리소스 모니터링 유틸리티 (대회 환경 리소스 측정용)\n",
    "# ============================================================\n",
    "try:\n",
    "    import psutil\n",
    "    PSUTIL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PSUTIL_AVAILABLE = False\n",
    "    print(\"⚠️  psutil not available - CPU/RAM monitoring disabled\")\n",
    "\n",
    "class ResourceMonitor:\n",
    "    \"\"\"대회 환경의 리소스 사용량을 측정하는 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        self.is_cuda = device == 'cuda' and torch.cuda.is_available()\n",
    "        self.process = psutil.Process() if PSUTIL_AVAILABLE else None\n",
    "        \n",
    "        # 통계 저장\n",
    "        self.stats = {\n",
    "            'gpu_memory_peak': 0,\n",
    "            'gpu_memory_current': 0,\n",
    "            'cpu_memory_peak': 0,\n",
    "            'cpu_memory_current': 0,\n",
    "            'inference_times': [],\n",
    "            'preprocessing_times': [],\n",
    "            'total_files_processed': 0\n",
    "        }\n",
    "        \n",
    "    def get_model_size(self, model):\n",
    "        \"\"\"모델의 파라미터 수와 메모리 크기 계산\"\"\"\n",
    "        param_count = sum(p.numel() for p in model.parameters())\n",
    "        param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "        buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "        total_size_mb = (param_size + buffer_size) / (1024 ** 2)\n",
    "        \n",
    "        return {\n",
    "            'total_params': param_count,\n",
    "            'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "            'size_mb': total_size_mb\n",
    "        }\n",
    "    \n",
    "    def get_gpu_memory(self):\n",
    "        \"\"\"GPU 메모리 사용량 조회 (MB 단위)\"\"\"\n",
    "        if not self.is_cuda:\n",
    "            return {'allocated': 0, 'reserved': 0, 'max_allocated': 0}\n",
    "        \n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated() / (1024 ** 2),  # MB\n",
    "            'reserved': torch.cuda.memory_reserved() / (1024 ** 2),\n",
    "            'max_allocated': torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "        }\n",
    "    \n",
    "    def get_cpu_memory(self):\n",
    "        \"\"\"CPU 메모리 사용량 조회 (MB 단위)\"\"\"\n",
    "        if not PSUTIL_AVAILABLE:\n",
    "            return {'rss': 0, 'vms': 0, 'percent': 0}\n",
    "        \n",
    "        mem_info = self.process.memory_info()\n",
    "        return {\n",
    "            'rss': mem_info.rss / (1024 ** 2),  # MB (실제 물리 메모리)\n",
    "            'vms': mem_info.vms / (1024 ** 2),  # MB (가상 메모리)\n",
    "            'percent': self.process.memory_percent()\n",
    "        }\n",
    "    \n",
    "    def get_gpu_utilization(self):\n",
    "        \"\"\"GPU 사용률 조회 (nvidia-smi 필요)\"\"\"\n",
    "        if not self.is_cuda:\n",
    "            return None\n",
    "        try:\n",
    "            import subprocess\n",
    "            result = subprocess.run(\n",
    "                ['nvidia-smi', '--query-gpu=utilization.gpu,temperature.gpu', '--format=csv,noheader,nounits'],\n",
    "                capture_output=True, text=True, timeout=1\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                util, temp = result.stdout.strip().split(',')\n",
    "                return {'utilization': float(util), 'temperature': float(temp)}\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def update_stats(self):\n",
    "        \"\"\"현재 리소스 사용량 업데이트\"\"\"\n",
    "        # GPU 메모리\n",
    "        gpu_mem = self.get_gpu_memory()\n",
    "        self.stats['gpu_memory_current'] = gpu_mem['allocated']\n",
    "        self.stats['gpu_memory_peak'] = max(self.stats['gpu_memory_peak'], gpu_mem['max_allocated'])\n",
    "        \n",
    "        # CPU 메모리\n",
    "        cpu_mem = self.get_cpu_memory()\n",
    "        self.stats['cpu_memory_current'] = cpu_mem['rss']\n",
    "        self.stats['cpu_memory_peak'] = max(self.stats['cpu_memory_peak'], cpu_mem['rss'])\n",
    "    \n",
    "    def log_inference_time(self, elapsed_time):\n",
    "        \"\"\"추론 시간 기록\"\"\"\n",
    "        self.stats['inference_times'].append(elapsed_time)\n",
    "        self.stats['total_files_processed'] += 1\n",
    "    \n",
    "    def log_preprocessing_time(self, elapsed_time):\n",
    "        \"\"\"전처리 시간 기록\"\"\"\n",
    "        self.stats['preprocessing_times'].append(elapsed_time)\n",
    "    \n",
    "    def print_current_status(self, prefix=\"\"):\n",
    "        \"\"\"현재 리소스 상태 출력\"\"\"\n",
    "        gpu_mem = self.get_gpu_memory()\n",
    "        cpu_mem = self.get_cpu_memory()\n",
    "        gpu_util = self.get_gpu_utilization()\n",
    "        \n",
    "        print(f\"\\n{prefix}[Resource Status]\")\n",
    "        print(f\"  GPU Memory: {gpu_mem['allocated']:.1f} MB (Peak: {gpu_mem['max_allocated']:.1f} MB)\")\n",
    "        print(f\"  CPU Memory: {cpu_mem['rss']:.1f} MB ({cpu_mem['percent']:.1f}%)\")\n",
    "        if gpu_util:\n",
    "            print(f\"  GPU Util: {gpu_util['utilization']:.1f}% | Temp: {gpu_util['temperature']:.1f}°C\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"최종 리소스 사용량 요약 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESOURCE USAGE SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 모델 정보는 별도로 출력되므로 생략\n",
    "        \n",
    "        # GPU 메모리\n",
    "        print(f\"\\n[GPU Memory (VRAM)]\")\n",
    "        print(f\"  Peak Usage: {self.stats['gpu_memory_peak']:.1f} MB\")\n",
    "        print(f\"  Current: {self.stats['gpu_memory_current']:.1f} MB\")\n",
    "        \n",
    "        # CPU 메모리\n",
    "        print(f\"\\n[CPU Memory (RAM)]\")\n",
    "        print(f\"  Peak Usage: {self.stats['cpu_memory_peak']:.1f} MB\")\n",
    "        print(f\"  Current: {self.stats['cpu_memory_current']:.1f} MB\")\n",
    "        \n",
    "        # 처리 시간 통계\n",
    "        if self.stats['inference_times']:\n",
    "            inf_times = self.stats['inference_times']\n",
    "            print(f\"\\n[Inference Time Per File]\")\n",
    "            print(f\"  Mean: {np.mean(inf_times)*1000:.1f} ms\")\n",
    "            print(f\"  Median: {np.median(inf_times)*1000:.1f} ms\")\n",
    "            print(f\"  Min: {np.min(inf_times)*1000:.1f} ms\")\n",
    "            print(f\"  Max: {np.max(inf_times)*1000:.1f} ms\")\n",
    "            print(f\"  Std: {np.std(inf_times)*1000:.1f} ms\")\n",
    "        \n",
    "        if self.stats['preprocessing_times']:\n",
    "            prep_times = self.stats['preprocessing_times']\n",
    "            print(f\"\\n[Preprocessing Time Per File]\")\n",
    "            print(f\"  Mean: {np.mean(prep_times)*1000:.1f} ms\")\n",
    "            print(f\"  Median: {np.median(prep_times)*1000:.1f} ms\")\n",
    "        \n",
    "        # 처리량\n",
    "        if self.stats['inference_times']:\n",
    "            total_time = sum(self.stats['inference_times'])\n",
    "            throughput = len(self.stats['inference_times']) / total_time if total_time > 0 else 0\n",
    "            print(f\"\\n[Throughput]\")\n",
    "            print(f\"  Files/sec: {throughput:.2f}\")\n",
    "            print(f\"  Total files: {self.stats['total_files_processed']}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import mediapipe as mp\n",
    "    MEDIAPIPE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MEDIAPIPE_AVAILABLE = False\n",
    "\n",
    "# 커스텀 모델 import\n",
    "sys.path.insert(0, './models')\n",
    "from vit.stv_transformer_hybrid import vit_base_r50_s16_224_with_recons_iafa\n",
    "\n",
    "# Simplified MMDet - 학습된 backbone + head만 사용\n",
    "class MMDet_Simplified(nn.Module):\n",
    "    \"\"\"학습된 backbone + head만 사용하는 안전한 구조\"\"\"\n",
    "    def __init__(self, window_size=10):\n",
    "        super(MMDet_Simplified, self).__init__()\n",
    "        self.backbone = vit_base_r50_s16_224_with_recons_iafa(\n",
    "            window_size=window_size, \n",
    "            pretrained=False, \n",
    "            ckpt_path=None,\n",
    "            num_classes=0,      # classifier 제거\n",
    "            global_pool=''      # pooling 제거\n",
    "        )\n",
    "        self.head = nn.Linear(768, 2)  # checkpoint에서 로드 가능\n",
    "                \n",
    "    def forward(self, x_input):\n",
    "        x_original, x_recons = x_input\n",
    "        # Backbone features 추출\n",
    "        x_st = self.backbone.forward_features(x_input)  # (B, T, 768)\n",
    "        # Temporal pooling\n",
    "        x_pooled = torch.mean(x_st, dim=1)              # (B, 768)\n",
    "        # Classification\n",
    "        out = self.head(x_pooled)                       # (B, 2)\n",
    "        return out\n",
    "\n",
    "### 추론 환경 경로 설정\n",
    "model_weights_path = \"./model/deep-fake-detector-v2-model/MM-Det/current_model.pth\"\n",
    "test_dataset_path = Path(\"./data\")\n",
    "output_csv_path = Path(\"submission.csv\")\n",
    "\n",
    "# --- 유틸리티 함수 ---\n",
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\"}\n",
    "VIDEO_EXTS = {\".avi\", \".mp4\"}\n",
    "\n",
    "FACE_DETECTOR = dlib.get_frontal_face_detector()\n",
    "HAAR_FACE_CASCADE = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "if MEDIAPIPE_AVAILABLE:\n",
    "    try:\n",
    "        mp_face_detection = mp.solutions.face_detection\n",
    "        MEDIAPIPE_DETECTOR = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.3)\n",
    "    except:\n",
    "        MEDIAPIPE_DETECTOR = None\n",
    "        MEDIAPIPE_AVAILABLE = False\n",
    "else:\n",
    "    MEDIAPIPE_DETECTOR = None\n",
    "\n",
    "def get_boundingbox(face, width, height):\n",
    "    x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()\n",
    "    size_bb = int(max(x2 - x1, y2 - y1) * 1.3)\n",
    "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "    x1 = max(int(center_x - size_bb // 2), 0)\n",
    "    y1 = max(int(center_y - size_bb // 2), 0)\n",
    "    size_bb = min(width - x1, size_bb)\n",
    "    size_bb = min(height - y1, size_bb)\n",
    "    return x1, y1, size_bb\n",
    "\n",
    "def detect_face_dlib(image_np, target_size=(224, 224), resize_for_detection=320):\n",
    "    original_h, original_w = image_np.shape[:2]\n",
    "    if original_w > resize_for_detection:\n",
    "        scale = resize_for_detection / float(original_w)\n",
    "        resized_h = int(original_h * scale)\n",
    "        resized_np = cv2.resize(image_np, (resize_for_detection, resized_h), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        scale = 1.0\n",
    "        resized_np = image_np\n",
    "    faces = FACE_DETECTOR(resized_np, 1)\n",
    "    if not faces:\n",
    "        return None\n",
    "    face = max(faces, key=lambda rect: rect.width() * rect.height())\n",
    "    scaled_face_rect = dlib.rectangle(\n",
    "        left=int(face.left() / scale), top=int(face.top() / scale),\n",
    "        right=int(face.right() / scale), bottom=int(face.bottom() / scale)\n",
    "    )\n",
    "    x, y, size = get_boundingbox(scaled_face_rect, original_w, original_h)\n",
    "    cropped_np = image_np[y:y + size, x:x + size]\n",
    "    face_img = Image.fromarray(cropped_np).resize(target_size, Image.BICUBIC)\n",
    "    return face_img\n",
    "\n",
    "def detect_face_haar(image_np, target_size=(224, 224)):\n",
    "    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "    faces = HAAR_FACE_CASCADE.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4, minSize=(30, 30))\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    x, y, w, h = max(faces, key=lambda f: f[2] * f[3])\n",
    "    expansion = int(max(w, h) * 0.3)\n",
    "    x = max(0, x - expansion)\n",
    "    y = max(0, y - expansion)\n",
    "    w = min(image_np.shape[1] - x, w + 2 * expansion)\n",
    "    h = min(image_np.shape[0] - y, h + 2 * expansion)\n",
    "    cropped_np = image_np[y:y + h, x:x + w]\n",
    "    face_img = Image.fromarray(cropped_np).resize(target_size, Image.BICUBIC)\n",
    "    return face_img\n",
    "\n",
    "def detect_face_mediapipe(image_np, target_size=(224, 224)):\n",
    "    if MEDIAPIPE_DETECTOR is None:\n",
    "        return None\n",
    "    image_rgb = cv2.cvtColor(image_np, cv2.COLOR_RGB2RGB)\n",
    "    results = MEDIAPIPE_DETECTOR.process(image_rgb)\n",
    "    if not results.detections:\n",
    "        return None\n",
    "    detection = max(results.detections, key=lambda d: d.score[0])\n",
    "    h, w = image_np.shape[:2]\n",
    "    bbox = detection.location_data.relative_bounding_box\n",
    "    x1 = int(bbox.xmin * w)\n",
    "    y1 = int(bbox.ymin * h)\n",
    "    x2 = int((bbox.xmin + bbox.width) * w)\n",
    "    y2 = int((bbox.ymin + bbox.height) * h)\n",
    "    margin = int(max(x2 - x1, y2 - y1) * 0.1)\n",
    "    x1 = max(0, x1 - margin)\n",
    "    y1 = max(0, y1 - margin)\n",
    "    x2 = min(w, x2 + margin)\n",
    "    y2 = min(h, y2 + margin)\n",
    "    cropped_np = image_np[y1:y2, x1:x2]\n",
    "    if cropped_np.size == 0:\n",
    "        return None\n",
    "    face_img = Image.fromarray(cropped_np).resize(target_size, Image.BICUBIC)\n",
    "    return face_img\n",
    "\n",
    "def detect_and_crop_face_multi(image: Image.Image, target_size=(224, 224)):\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    if MEDIAPIPE_AVAILABLE:\n",
    "        try:\n",
    "            face_img = detect_face_mediapipe(image_np, target_size)\n",
    "            if face_img:\n",
    "                return face_img\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        face_img = detect_face_dlib(image_np, target_size)\n",
    "        if face_img:\n",
    "            return face_img\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        face_img = detect_face_haar(image_np, target_size)\n",
    "        if face_img:\n",
    "            return face_img\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    resized_img = Image.fromarray(image_np).resize(target_size, Image.BICUBIC)\n",
    "    return resized_img\n",
    "\n",
    "def process_video_frames(video_path, num_frames=10, max_duration=10):\n",
    "    face_images = []\n",
    "    cap = None\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            return face_images\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0:\n",
    "            return face_images\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if fps > 0:\n",
    "            max_frames = int(fps * max_duration)\n",
    "            total_frames = min(total_frames, max_frames)\n",
    "        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            face_img = detect_and_crop_face_multi(image)\n",
    "            if face_img:\n",
    "                face_images.append(face_img)\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        if cap is not None:\n",
    "            cap.release()\n",
    "    return face_images\n",
    "\n",
    "# --- 메인 처리 로직 ---\n",
    "if __name__ == \"__main__\" or True:  # Jupyter에서도 작동하도록\n",
    "    print(\"=\"*60)\n",
    "    print(\"Starting inference (Simplified MMDet - backbone + head only)...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # GPU 사용 가능 여부 확인 및 device 설정\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # 리소스 모니터 초기화\n",
    "    resource_monitor = ResourceMonitor(device=device)\n",
    "    print(f\"✓ Resource monitoring enabled (Device: {device})\")\n",
    "\n",
    "    print(f\"\\nLoading MMDet_Simplified model...\")\n",
    "    model = MMDet_Simplified(window_size=10)\n",
    "\n",
    "    # Checkpoint 로드\n",
    "    checkpoint = torch.load(model_weights_path, map_location='cpu')\n",
    "    new_state_dict = {}\n",
    "    for k, v in checkpoint.items():\n",
    "        if k.startswith('module.'):\n",
    "            k = k[7:]\n",
    "        # backbone과 head만 로드 (학습된 가중치만 사용)\n",
    "        if any(k.startswith(prefix) for prefix in ['backbone', 'head']):\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "    load_result = model.load_state_dict(new_state_dict, strict=False)\n",
    "    print(f\"✓ Model loaded: missing={len(load_result.missing_keys)}, unexpected={len(load_result.unexpected_keys)}\")\n",
    "\n",
    "    # 모델을 device로 이동\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"✓ MMDet_Simplified ready on {device}\")\n",
    "    \n",
    "    # 모델 크기 측정\n",
    "    model_info = resource_monitor.get_model_size(model)\n",
    "    print(f\"\\n[Model Information]\")\n",
    "    print(f\"  Total Parameters: {model_info['total_params']:,}\")\n",
    "    print(f\"  Trainable Parameters: {model_info['trainable_params']:,}\")\n",
    "    print(f\"  Model Size: {model_info['size_mb']:.2f} MB\")\n",
    "    \n",
    "    # 모델 로딩 후 초기 메모리 상태\n",
    "    resource_monitor.update_stats()\n",
    "    resource_monitor.print_current_status(prefix=\"After Model Loading - \")\n",
    "\n",
    "    # 이미지 전처리\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # 모델 sanity check: 더미 입력으로 추론 테스트\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 10, 3, 224, 224).to(device)\n",
    "            dummy_output = model((dummy_input, dummy_input))\n",
    "            print(f\"✓ Model sanity check passed (output shape: {dummy_output.shape})\")\n",
    "            del dummy_input, dummy_output\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Model sanity check FAILED: {type(e).__name__}: {str(e)[:100]}\")\n",
    "        print(f\"  This model may not work properly!\")\n",
    "\n",
    "    # CSV 파일 헤더 먼저 작성 (baseline 방식)\n",
    "    with open(output_csv_path, mode=\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"filename\", \"label\"])\n",
    "    print(f\"✓ CSV header created\")\n",
    "\n",
    "    # 평가 데이터 파일 목록\n",
    "    files = [p for p in sorted(test_dataset_path.iterdir()) if p.is_file()]\n",
    "    total_files = len(files)\n",
    "    print(f\"✓ Test data files: {total_files}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    num_frames_to_extract = 10\n",
    "    results = []\n",
    "    error_count = 0\n",
    "    error_details = []  # 디버깅용 에러 정보 저장\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 파일 처리\n",
    "    for idx, file_path in enumerate(tqdm(files, desc=\"Processing\", ncols=80)):\n",
    "        face_images = []\n",
    "        ext = file_path.suffix.lower()\n",
    "        predicted_class = 0\n",
    "        \n",
    "        # 파일별 처리 시간 측정\n",
    "        file_start_time = time.time()\n",
    "        prep_start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            if ext in IMAGE_EXTS:\n",
    "                image = Image.open(file_path)\n",
    "                face_img = detect_and_crop_face_multi(image)\n",
    "                if face_img:\n",
    "                    face_images = [face_img] * 10\n",
    "\n",
    "            elif ext in VIDEO_EXTS:\n",
    "                face_images = process_video_frames(file_path, num_frames_to_extract, max_duration=10)\n",
    "                if len(face_images) > 0 and len(face_images) < 10:\n",
    "                    last_frame = face_images[-1]\n",
    "                    face_images.extend([last_frame] * (10 - len(face_images)))\n",
    "                elif len(face_images) == 0:\n",
    "                    try:\n",
    "                        cap = cv2.VideoCapture(str(file_path))\n",
    "                        ret, frame = cap.read()\n",
    "                        cap.release()\n",
    "                        if ret:\n",
    "                            fallback_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                            face_img = detect_and_crop_face_multi(fallback_image)\n",
    "                            face_images = [face_img] * 10\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # 전처리 시간 기록\n",
    "            prep_time = time.time() - prep_start_time\n",
    "            resource_monitor.log_preprocessing_time(prep_time)\n",
    "\n",
    "            # 추론 수행\n",
    "            if len(face_images) >= 10:\n",
    "                inference_start_time = time.time()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    batch = face_images[:10]\n",
    "                    img_tensors = torch.stack([transform(img) for img in batch]).unsqueeze(0).to(device)\n",
    "                    x_original = img_tensors\n",
    "                    x_recons = img_tensors\n",
    "                    \n",
    "                    logits = model((x_original, x_recons))\n",
    "                    probs = F.softmax(logits, dim=1)\n",
    "                    predicted_class = torch.argmax(probs).item()\n",
    "                    \n",
    "                    # GPU 메모리 정리\n",
    "                    del img_tensors, x_original, x_recons, logits, probs\n",
    "                    if device == \"cuda\":\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # 추론 시간 기록\n",
    "                inference_time = time.time() - inference_start_time\n",
    "                resource_monitor.log_inference_time(inference_time)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            # stdout에 출력하지 않고 파일로 저장 (처음 10개만)\n",
    "            if len(error_details) < 10:\n",
    "                error_details.append({\n",
    "                    'filename': file_path.name,\n",
    "                    'error_type': type(e).__name__,\n",
    "                    'error_msg': str(e)[:200]  # 메시지 길이 제한\n",
    "                })\n",
    "            predicted_class = 0\n",
    "\n",
    "        # 결과를 리스트에 저장 (baseline 방식)\n",
    "        results.append([file_path.name, int(predicted_class)])\n",
    "\n",
    "        # 주기적으로 리소스 상태 업데이트 및 출력\n",
    "        if (idx + 1) % 500 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            speed = (idx + 1) / elapsed\n",
    "            remaining = (total_files - idx - 1) / speed if speed > 0 else 0\n",
    "            print(f\"\\n[Progress] {idx+1}/{total_files} | Speed: {speed:.2f} files/s | ETA: {remaining/60:.1f}m\")\n",
    "            \n",
    "            # 리소스 상태 업데이트 및 출력\n",
    "            resource_monitor.update_stats()\n",
    "            resource_monitor.print_current_status(prefix=f\"[{idx+1}/{total_files}] \")\n",
    "\n",
    "    # 최종 통계\n",
    "    elapsed_total = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"✓ Inference completed in {elapsed_total/60:.1f} minutes\")\n",
    "    print(f\"  Total processed: {len(results)} files\")\n",
    "    print(f\"  Speed: {len(results)/elapsed_total:.2f} files/sec\")\n",
    "    print(f\"  Errors: {error_count} files\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # CSV에 결과 추가 (append mode, baseline 방식)\n",
    "    with open(output_csv_path, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for row in results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # CSV 검증 및 상세 분석\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CSV Validation:\")\n",
    "    print(f\"  Expected: {total_files} files\")\n",
    "    \n",
    "    # CSV 파일 읽어서 확인\n",
    "    with open(output_csv_path, mode=\"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = list(reader)\n",
    "        header = rows[0]\n",
    "        data_rows_list = rows[1:]\n",
    "        data_rows = len(data_rows_list)\n",
    "    \n",
    "    print(f\"  Created: {data_rows} rows (excluding header)\")\n",
    "    print(f\"  Header: {header}\")\n",
    "    \n",
    "    if data_rows == total_files:\n",
    "        print(\"  ✓ CSV validation PASSED!\")\n",
    "    else:\n",
    "        print(f\"  ✗ FAILED! Missing {total_files - data_rows} rows\")\n",
    "    print(f\"  File size: {output_csv_path.stat().st_size:,} bytes\")\n",
    "    \n",
    "    # 상세 분석\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CSV Content Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Label 분포\n",
    "    label_counts = {}\n",
    "    for row in data_rows_list:\n",
    "        if len(row) >= 2:\n",
    "            label = row[1]\n",
    "            label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    print(f\"Label distribution: {label_counts}\")\n",
    "    \n",
    "    # 2. 샘플 데이터 (처음 5개, 마지막 5개)\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    for i, row in enumerate(data_rows_list[:5], 1):\n",
    "        print(f\"  [{i}] {row}\")\n",
    "    \n",
    "    print(f\"\\nLast 5 rows:\")\n",
    "    for i, row in enumerate(data_rows_list[-5:], data_rows-4):\n",
    "        print(f\"  [{i}] {row}\")\n",
    "    \n",
    "    # 3. 중복 체크\n",
    "    filenames_in_csv = [row[0] for row in data_rows_list if len(row) >= 1]\n",
    "    unique_filenames = set(filenames_in_csv)\n",
    "    if len(filenames_in_csv) != len(unique_filenames):\n",
    "        duplicates = len(filenames_in_csv) - len(unique_filenames)\n",
    "        print(f\"\\n⚠️  WARNING: {duplicates} duplicate filename(s) found!\")\n",
    "    else:\n",
    "        print(f\"\\n✓ No duplicates (all {len(unique_filenames)} filenames are unique)\")\n",
    "    \n",
    "    # 4. 파일명 매칭 체크\n",
    "    expected_filenames = set([p.name for p in files])\n",
    "    csv_filenames = set(filenames_in_csv)\n",
    "    missing_in_csv = expected_filenames - csv_filenames\n",
    "    extra_in_csv = csv_filenames - expected_filenames\n",
    "    \n",
    "    if missing_in_csv:\n",
    "        print(f\"\\n⚠️  Files missing in CSV: {len(missing_in_csv)}\")\n",
    "        for fn in list(missing_in_csv)[:5]:\n",
    "            print(f\"    - {fn}\")\n",
    "        if len(missing_in_csv) > 5:\n",
    "            print(f\"    ... and {len(missing_in_csv)-5} more\")\n",
    "    \n",
    "    if extra_in_csv:\n",
    "        print(f\"\\n⚠️  Extra files in CSV: {len(extra_in_csv)}\")\n",
    "        for fn in list(extra_in_csv)[:5]:\n",
    "            print(f\"    - {fn}\")\n",
    "        if len(extra_in_csv) > 5:\n",
    "            print(f\"    ... and {len(extra_in_csv)-5} more\")\n",
    "    \n",
    "    if not missing_in_csv and not extra_in_csv:\n",
    "        print(f\"\\n✓ All filenames match perfectly!\")\n",
    "    \n",
    "    # 5. 데이터 타입 체크\n",
    "    invalid_rows = []\n",
    "    for i, row in enumerate(data_rows_list, 1):\n",
    "        if len(row) != 2:\n",
    "            invalid_rows.append((i, row, f\"Wrong column count: {len(row)}\"))\n",
    "        elif row[1] not in ['0', '1']:\n",
    "            invalid_rows.append((i, row, f\"Invalid label: {row[1]}\"))\n",
    "    \n",
    "    if invalid_rows:\n",
    "        print(f\"\\n⚠️  Invalid rows found: {len(invalid_rows)}\")\n",
    "        for idx, row, reason in invalid_rows[:5]:\n",
    "            print(f\"    Row {idx}: {row} - {reason}\")\n",
    "        if len(invalid_rows) > 5:\n",
    "            print(f\"    ... and {len(invalid_rows)-5} more\")\n",
    "    else:\n",
    "        print(f\"\\n✓ All rows have valid format (2 columns, labels are 0 or 1)\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 추가 검증: 성공한 추론이 있는지 확인\n",
    "    if error_count == total_files:\n",
    "        print(f\"⚠️  WARNING: ALL files failed! Check details below\")\n",
    "    elif error_count > total_files * 0.5:\n",
    "        print(f\"⚠️  WARNING: High error rate ({error_count}/{total_files})\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 디버그 정보 출력 (CSV 검증 후 맨 마지막)\n",
    "    if error_details:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DEBUG: Sample Issues (first 10)\")\n",
    "        print(\"=\"*60)\n",
    "        for i, err in enumerate(error_details, 1):\n",
    "            print(f\"\\n[{i}] {err['filename']}\")\n",
    "            print(f\"    Type: {err['error_type']}\")\n",
    "            print(f\"    Info: {err['error_msg']}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    # 최종 리소스 사용량 요약\n",
    "    resource_monitor.update_stats()\n",
    "    resource_monitor.print_summary()\n",
    "    \n",
    "    # 모델 규모 확장 시 예상 리소스 사용량 가이드\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESOURCE SCALING GUIDE (모델 규모 확장 시 예상치)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n현재 모델: ViT-Base (MMDet_Simplified)\")\n",
    "    print(f\"  - Parameters: {model_info['total_params']:,}\")\n",
    "    print(f\"  - Model Size: {model_info['size_mb']:.1f} MB\")\n",
    "    print(f\"  - Peak VRAM: {resource_monitor.stats['gpu_memory_peak']:.1f} MB\")\n",
    "    print(f\"  - Peak RAM: {resource_monitor.stats['cpu_memory_peak']:.1f} MB\")\n",
    "    \n",
    "    # 더 큰 모델로 확장 시 예상치 계산\n",
    "    current_params = model_info['total_params']\n",
    "    current_vram = resource_monitor.stats['gpu_memory_peak']\n",
    "    \n",
    "    print(\"\\n예상 리소스 (모델 크기별):\")\n",
    "    print(\"┌─────────────────┬──────────────┬───────────────┬──────────────┐\")\n",
    "    print(\"│ Model           │ Parameters   │ Estimated VRAM│ Batch Size   │\")\n",
    "    print(\"├─────────────────┼──────────────┼───────────────┼──────────────┤\")\n",
    "    print(f\"│ ViT-Base (현재) │ {current_params/1e6:6.1f}M      │ {current_vram:6.1f} MB     │ 1 (현재)     │\")\n",
    "    \n",
    "    # ViT-Large 예상 (ViT-Base의 약 3.4배 파라미터)\n",
    "    large_params = current_params * 3.4\n",
    "    large_vram = current_vram * 2.5  # 메모리는 비선형적으로 증가\n",
    "    print(f\"│ ViT-Large       │ {large_params/1e6:6.1f}M      │ {large_vram:6.1f} MB     │ 1            │\")\n",
    "    \n",
    "    # Full MM-Det (LLaVA 추가)\n",
    "    full_vram = current_vram + 10000  # LLaVA 7B 모델 추가 (~10GB)\n",
    "    print(f\"│ Full MM-Det     │ {(current_params + 7e9)/1e6:6.1f}M      │ {full_vram:6.1f} MB     │ 1            │\")\n",
    "    \n",
    "    # ViT-Huge\n",
    "    huge_params = current_params * 7.0\n",
    "    huge_vram = current_vram * 4.0\n",
    "    print(f\"│ ViT-Huge        │ {huge_params/1e6:6.1f}M      │ {huge_vram:6.1f} MB     │ 1            │\")\n",
    "    \n",
    "    print(\"└─────────────────┴──────────────┴───────────────┴──────────────┘\")\n",
    "    \n",
    "    print(\"\\n⚠️  참고사항:\")\n",
    "    print(\"  - VRAM 추정치는 batch_size=1, fp32 기준입니다\")\n",
    "    print(\"  - 실제 사용량은 데이터 형태, 최적화 기법에 따라 달라질 수 있습니다\")\n",
    "    print(\"  - Mixed Precision (FP16) 사용 시 VRAM을 약 40~50% 절감할 수 있습니다\")\n",
    "    print(\"  - Batch size를 늘리면 VRAM이 비례하여 증가합니다\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119d40d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f895e41",
   "metadata": {},
   "source": [
    "#### 3. `aif.submit()` 함수를 호출하여 최종 결과를 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410d97e",
   "metadata": {},
   "source": [
    "**※주의** : task별, 참가자별로 key가 다릅니다. 잘못 입력하지 않도록 유의바랍니다.\n",
    "- key는 대회 페이지 [베이스라인 코드](https://aifactory.space/task/9197/baseline) 탭에 기재된 가이드라인을 따라 task 별로 확인하실 수 있습니다.\n",
    "- key가 틀리면 제출이 진행되지 않거나 잘못 제출되므로 task에 맞는 자신의 key를 사용해야 합니다.\n",
    "-  **NOTE** : 이번 경진대회에서는 3개의 CUDA 버전을 지원하며, 각 CUDA 버전에 따라 task key가 상이합니다. 함수를 실행하기 전에 현재 key가 제출하고자 하는 CUDA 환경에 대한 key인지 반드시 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4df34c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file : task\n",
      "jupyter notebook\n",
      "제출 완료\n",
      "1657.3978803157806\n"
     ]
    }
   ],
   "source": [
    "import aifactory.score as aif\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "#-----------------------------------------------------#\n",
    "aif.submit(model_name=\"ViT-DeepfakeDetector\",\n",
    "    key=\"cae0fbcb-0410-4084-a308-21c98d8d886b\"\n",
    ")\n",
    "#-----------------------------------------------------#\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b52f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ac2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
